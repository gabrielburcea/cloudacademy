{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27bbe674",
   "metadata": {},
   "source": [
    "###Â Exercises\n",
    "\n",
    "\n",
    "#### Exercise 1 \n",
    "\n",
    "In exercise one of section four, we're asked to:\n",
    "1. predict whether or not some people will be diagnosed diabetes from a set of variables of exams. \n",
    "2. So this is the population of Pima Indians. It's a very famous dataset that we got from UCI and it contains information about the patients:\n",
    "* including pregnancies, glucose, blood pressure, and \n",
    "* then a few other medical examinations, \n",
    "* and the last column is the outcome which is a binary variable. \n",
    "\n",
    "So it's a classification problem:\n",
    "* and you're guided through a series of steps that go from loading the data, creating a histogram to inspect the features, \n",
    "* and exploring the correlations between the features and the outcome column. \n",
    "\n",
    "We suggest using the seaborn pairplot, but you can also draw a heat map as we saw in the lecture. \n",
    "\n",
    "Then there are a few open questions. \n",
    "* Do features need standardization? And if so, what kind? \n",
    "* Are we gonna use MinMax or standard? \n",
    "* And then finally, prepare x and y using a machine learning model. \n",
    "* Do you need dummy columns? \n",
    "* And make sure you define your target variable. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2f0269",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904fcdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dataset/diabets.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcff6170",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = df.hist(figsize=(12,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478190bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, hue = 'Outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87c6ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(), annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd79422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f5c564",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0410c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f75a70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X = sc.fit_transformer(df.drop('Outcome', axis = 1))\n",
    "y = df['Outcome'].value\n",
    "y_cat = to_categorical(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8cb49d",
   "metadata": {},
   "source": [
    "#### Exercise 2 \n",
    "\n",
    "In exercise two, \n",
    "* we build a fully connected neural network model to predict the diabetes patients that we've loaded and prepared in exercise one. \n",
    "* So we're guided through a series of step: \n",
    "1. we start by splitting the data into train test split, \n",
    "2. then, we define a sequential model, with at least one inner layer. \n",
    "3. So, to build this model, we'll have to make a few choices: \n",
    "       * What the size of the input is, \n",
    "       * how many nodes we will use in each layer, \n",
    "       * the size of the outputs, \n",
    "       * and then what activation functions we will use in the inner layers, and, \n",
    "       * what activation functions we're gonna use at the output. \n",
    "       * Also, what loss function we will use, \n",
    "       * and what optimizer we will use. \n",
    "       * Finally, we'll fit the model on the training set, using a validation split of 0.1, and we'll test the trained model on the test data from the train test split. \n",
    "       * Finally we check the accuracy score, the confusion matrix, and the classification report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7421d13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f547578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4031c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_cat, \n",
    "                                                   random_state = 22, \n",
    "                                                   test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b9e886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c118bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape = (8, ), activation = 'relu'))\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "model.compile(Adam(lr = 0.05), \n",
    "             loss = 'categorical_crossentropy', \n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd71e3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs = 20, verbose = 2, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aca025",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee86450",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d981d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_class = np.argmax(y_test, axis = 1)\n",
    "y_pred_class = np.armax(y_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6046313a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae03715",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_test_class).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9dee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test_class, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67665ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_class, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a221158",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test_class, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b2bfe5",
   "metadata": {},
   "source": [
    "#### Exercise 3 \n",
    "\n",
    "I will review exercise three in section four.\n",
    "This exercise asks you to compare your results on the Pima Indian data set classification, with the results presented in a notebook on the Kaggle website. In this website, they use different machine learning techniques and they built a model to predict the same outcome you are trying to predict. \n",
    "\n",
    "So the question you're asked is are neural networks better or worse in this particular case? \n",
    "\n",
    "I will be comparing my results also with the few models from psychic learn for example: \n",
    "* a support vector machine or/and random forest, \n",
    "* and on the exact same train/test split. \n",
    "\n",
    "So I'm not going to tell if the performance is worse or better, that's for you to find out. Also, we ask you to try restricting your features to only four features like in the suggested notebook. \n",
    "\n",
    "And how does the model performance change? \n",
    "1. You can test this for your model, the neural network, \n",
    "2. but also for the models like the random forest and the support vector machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b8658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "for mod in [RandomForestClassifier(), SVC(), GaussianNB()]:\n",
    "    mod.fit(X_train, y_train[:, 1])\n",
    "    y_pred = mod.predict(X_test)\n",
    "    print(\"=\"*80)\n",
    "    print(mod)\n",
    "    print('Accuracy score: {:0.3}'.format(accuracy_score(y_test_class, \n",
    "                                                         y_pred)))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test_class, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6012a889",
   "metadata": {},
   "source": [
    "#### Exercise 4 - go on Tensorflow playground on a web-browser\n",
    "\n",
    "This exercise is about Tensorflow playground. Tensorflow playground is a very nice web application from the guys at Tensorflow that provides an interactive environment where you can play with simple fully connected neural nets on very simple data sets. These are two featured data sets where you have to separate groups of points that are blue and orange. So play with it a few minutes. There's no real goal. You don't need to understand the meaning of every knob and button but just to develop a feeling, an intuition about how things work. So there's no real challenge here just feel free to explore, play with it and see what you find"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
